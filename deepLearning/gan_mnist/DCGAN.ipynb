{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Input to the Generator - a random vector z (noise) usually with length around 100). The output is set to the discriminator, which is why we need **to upsample the vector to fit the shape of the real image** (just like in autoencoders (actually in decoder part).\n",
    "\n",
    "**Transposed convolution** \n",
    "\n",
    "Used to upsample the layers from shallow and wide to narrow and deep. To obtain it we set the stride to 2, what means that when the kernal moves 1 pixel on the input layer, the patch moves 2 pixels on the output layer. Taking into account padding set as **same**, the output layer dimension result in double of the input.\n",
    "\n",
    "Input vector z is connected to a fully connected layer and reshape it to 4x4x(layer depth) and then we perform upsampling to build a stack of convolutional layers. The last layer has a dimension of (real image height)x(real image width)x3.Depth equal to 3 corresponds to 3 color channels.\n",
    "\n",
    "No maxpool layers, no fully connected layers. ReLU activation function is often used on each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator \n",
    "\n",
    "The Discriminator is a convolutional network with a fully connected layer in the end, followed by a sigmod activation function. No maxpool layers. Downsampling is performed completly with strided convolution. Layers have reLu activation layers and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Performed on all convolutional layers beside the last one. It scales the layer inputs to have a mean of 0 and a variance of 1. It speeds up the training and reduced the problem due to poor parameter initialization.\n",
    "\n",
    "It actually slows down each iteration because we add additional calculation to perform, however the model converges faster, what decreases the overall training time. \n",
    "\n",
    "It allows to use higher learning rate, what again speeds up the training process. It is possible becuase the gradients do not get smaller with each layer during back propagation process.\n",
    "\n",
    "It is easier to initialize weights. It is possible to use sigmoid or ReLU, becuase the input is now controlled and  we do not face vanishing gradient problem. \n",
    "\n",
    "Having all these advantages, it is simply easier to create a neural network and faster to train it. \n",
    "\n",
    "Moreover, batch normalization add a bit of noise, which works as a regularization. Point for batch normalization again! In some cases it can even substitute part of a dropout layer. \n",
    "\n",
    "Batch normalization uses weights, but does not use bias because of the presence of beta and gamma variables.\n",
    "\n",
    "We apply activation function **after** output normalization. \n",
    "\n",
    "tf.layers.batch_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
